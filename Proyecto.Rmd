---
title: "R Notebook"
output: html_notebook
---


```{r}
library("readxl")
x_raw <- read_excel("./kcmillersean-billboard-hot-100-1958-2017/original/Hot 100 Audio Features.xlsx")
sum(is.na(x_raw$spotify_track_explicit))
x_raw
```
We eliminate the rows where the target variable is missing.

```{r}
billboard <- read.csv("./kcmillersean-billboard-hot-100-1958-2017/original/Hot Stuff.csv",header = TRUE)
billboard
```


```{r}
x_full <- x_raw[!is.na(x_raw$danceability),]
x_full  <- x_full[!is.na(x_full$spotify_track_explicit),]
sum(is.na(x_full$spotify_track_explicit))
x_full
```

```{r}
explicit <- x_full$spotify_track_explicit
x <- x_full[,c(9:22)] 
colnames(x)[1]<-"duration"
colnames(x)[2]<-"popularity"
x
sum(is.na(x))
```
We have to see what to do with mode and time_signature.

Quitar key y time_signature
```{r}
x<-x[,-c(5,14)]
```

```{r}
dim(x)
```

```{r}
# pairs(x[,1:6],col = ifelse(explicit,"red","blue"))
```

```{r}
C <-cor(x)
C
```

```{r}
for(i in 1 : (nrow(C) - 1)) {
  for(j in (i + 1) : ncol(C)) {
    if(abs(C[i, j]) >= 0.5) print(paste0(rownames(C)[i], " - ", colnames(C)[j]))
  }
}
```



```{r}
mu_exp = as.matrix(apply(x[explicit == TRUE,],2,mean))
mu_exp
```

```{r}
mu_nexp = as.matrix(apply(x[explicit == FALSE,],2,mean))
mu_nexp
```

```{r}
apply(x[explicit == TRUE,],2,var)
```

```{r}
apply(x[explicit == FALSE,],2,var)
```

Asumim que les variancies son diferents (s'hauria de fer box m test)

```{r}
sum(explicit == FALSE)
sum(explicit == TRUE)
```
```{r}
hist(x$loudness[explicit == FALSE],freq = FALSE,border = "green",ylim = c(0,0.2))
hist(x$loudness[explicit == TRUE],freq = FALSE,add  = TRUE,border = "red",ylim = c(0,0.2))


```

```{r}
hist(x$speechiness[explicit == FALSE],freq = FALSE,border = "green")
hist(x$speechiness[explicit == TRUE],freq = FALSE,add  = TRUE,border = "red")
```
```{r}
hist(x$danceability[explicit == FALSE],freq = FALSE,border = "green")
hist(x$danceability[explicit == TRUE],freq = FALSE,add  = TRUE,border = "red")
```
```{r}
hist(x$popularity[explicit == FALSE],freq = FALSE,border = "green",ylim = c(0,0.05))
hist(x$popularity[explicit == TRUE],freq = FALSE,add  = TRUE,border = "red",ylim = c(0,0.05))
```
```{r}
hist(x$valence[explicit == FALSE],freq = FALSE,border = "green")
hist(x$valence[explicit == TRUE],freq = FALSE,add  = TRUE,border = "red") 
```

```{r}
set.seed(123)
xstd = scale(x,scale = TRUE)
c = sample(1:nrow(xstd),1000)
D = dist(xstd[c,])
```



```{r}
hc.ward <- hclust(D,method="ward.D2")
plot(hc.ward,ylab="Distance",main="single linkage (weighted Euclidean)",
xlab="",hang=-1,las=1,cex.main=1)
clusters <- cutree(hc.ward, 2)
table(clusters,explicit[c])
```

```{r}
# LDA
library(MASS)
out <- lda(explicit~.,data=x)
plot(out)
```


```{r}
# PCA no separa
pca = princomp(x[c,],cor=TRUE)
plot(pca$scores[,1],pca$scores[,2],col = ifelse(explicit,"red","blue"),asp = 1)
```


## ClassificaciÃ³

ara ens falta fer learn + test i cross validation

```{r}
g = function(prior,S_inv,mu,x){
  x = t(as.matrix(x))
  g = log(prior) - log((2*pi)^(ncol(S_inv)/2)) -1/2*t(x-mu) %*% S_inv %*% (x-mu)
  return(g)
}

dichotomizer = function(xx,prior_exp,prior_nexp,S_exp_inv,S_nexp_inv,mu_exp,mu_nexp){
    class = rep(0, nrow(xx))
    for (i in 1:nrow(xx)){
        dico = g(prior_exp,S_exp_inv,mu_exp,xx[i,])-g(prior_nexp,S_nexp_inv,mu_nexp,xx[i,])
        if (dico > 0)class(i) = 1 
    }
    return(class)
} 

F1 = function(y_true,y_pred){
    positives = c(y_true == 1)
    true_positives = sum(positives) - sum((y_true[positives] - y_pred[positives])^2)
    prec = true_positives/sum(y_pred == 1)
    recall = true_positives/sum(y_true == 1)
    F1 = 2*(prec*recall)/(prec+recall)
    return (F1)
}
```

```{r}
mod <- glm(explicit~as.matrix(x),family=binomial(link = "logit"))
summary(mod)
```

```{r}
sum((round(predict(mod,type = "response"))-explicit)^2)/nrow(x)
```

```{r}
set.seed (4321)
N <- nrow(x)
learn <- sample(1:N, round(0.67*N))
xl = x[learn,]
yl = explicit[learn]
xtest = x[-learn,]
ytest = explicit[-learn]
```

```{r}
k=10
acum_error0 = 0
acum_error1 = 0
fold_size = nrow(xl) %/% k
for (i in 1:k){
    #Definim els talls que definiran els folds
    init = (i-1)*fold_size
    if(i == 10){
    end = nrow(xl)
    }else{
    end = init + fold_size
    }
    xtraining = xl[-c(init:end),]
    ytraining = yl[-c(init:end)]
    xvalidation = xl[c(init:end),]
    yvalidation = yl[c(init:end)]
    
    #Model0: dichotomizer assuming MVN:
    # Creem el dichotomizer
    # Primer creem les funcions discriminants
    prior_exp = 1/2#sum(ytraining == TRUE)/nrow(xtraining)
    prior_nexp = 1 - prior_exp
    S_exp = cov(xtraining[ytraining == TRUE,])
    S_nexp = cov(xtraining[ytraining == FALSE,])
    S_exp_inv = solve(S_exp)
    S_nexp_inv = solve(S_nexp)
    prediction0 = dichotomizer(xvalidation,prior_exp,prior_nexp,S_exp_inv,S_nexp_inv,mu_exp,mu_nexp)
    acum_error0 = acum_error0 + F1(as.numeric(yvalidation),prediction0)
    
    #Model1: glm with binomial family
    mod1 <- glm(ytraining ~ ., data=as.data.frame(xtraining),family=binomial)
    prediction1 = round(predict(mod1,ty="response",
    newdata = as.data.frame(xvalidation)))
    acum_error1 = acum_error1 + F1(as.numeric(yvalidation),prediction1)
}
error0 = acum_error0/k
error1 = acum_error1/k
error0
error1
```

```{r}
library(MLmetrics)
F1_Score(as.numeric(yvalidation),prediction1)
```

```{r}

```






